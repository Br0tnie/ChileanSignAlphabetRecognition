{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL220kF8S7j2"
      },
      "source": [
        "## Librerías\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_Dmn5kHS7j4"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "from datetime import timedelta, datetime\n",
        "import collections\n",
        "from tkinter import *\n",
        "import mediapipe as mp\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlZz80WsS7j5"
      },
      "outputs": [],
      "source": [
        "#import threading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUA3axUmS7j5"
      },
      "outputs": [],
      "source": [
        "#Instalar YOLO\n",
        "%pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cM2keByuS7j5"
      },
      "outputs": [],
      "source": [
        "import ultralytics\n",
        "from ultralytics import YOLO\n",
        "import sys\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSY3NkJHS7j5"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qB51PifS7j6"
      },
      "outputs": [],
      "source": [
        "mp_holistic = mp.solutions.holistic # Holistic model\n",
        "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DY4dZzBS7j6"
      },
      "outputs": [],
      "source": [
        "def mediapipe_detection(image, model):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
        "    image.flags.writeable = False                  # Image is no longer writeable\n",
        "    results = model.process(image)                 # Make prediction\n",
        "    image.flags.writeable = True                   # Image is now writeable\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
        "    return image, results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMJucwHUS7j6"
      },
      "outputs": [],
      "source": [
        "def draw_landmarks(image, results):\n",
        "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS) # Draw face connections\n",
        "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
        "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
        "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDWCZuYZS7j6"
      },
      "outputs": [],
      "source": [
        "def draw_styled_landmarks(image, results):\n",
        "    # Draw face connections\n",
        "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
        "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
        "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
        "                             )\n",
        "    # Draw pose connections\n",
        "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
        "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
        "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
        "                             )\n",
        "    # Draw left hand connections\n",
        "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
        "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
        "                             )\n",
        "    # Draw right hand connections\n",
        "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
        "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
        "                             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l48t25wJS7j6"
      },
      "outputs": [],
      "source": [
        "def extract_keypoints(results):\n",
        "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
        "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
        "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
        "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
        "    return np.concatenate([pose, lh, rh]) #face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-9Zr-R1S7j7"
      },
      "outputs": [],
      "source": [
        "# Path for exported data, numpy arrays\n",
        "#DATA_PATH = os.path.join('MP_Data')\n",
        "\n",
        "# Actions that we try to detect\n",
        "actions = np.array(['G', 'J','S', 'X', 'Z'])  #actions = np.array(['G', 'J', 'Ñ', 'S', 'X', 'Z']) #ESTO DEBERÍA IR\n",
        "\n",
        "# Thirty videos worth of data\n",
        "no_sequences = 30\n",
        "\n",
        "# Videos are going to be 30 frames in length\n",
        "sequence_length = 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7wECGyGS7j7"
      },
      "source": [
        "## Loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4Hvd3dpS7j7"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
        "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
        "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(actions.shape[0], activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mf-v_S4xS7j7"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "model.summary()\n",
        "model.load_weights('GJSXZ.h5') #VERIFICAR NOMBRE DEL MODELO DINAMICO A CARGAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kk9OEr61S7j8"
      },
      "outputs": [],
      "source": [
        "model_YOLO = YOLO('best.pt') #VERIFICAR NOMBRE DEL MODELO YOLO A CARGAR\n",
        "#model_YOLO.model.to(torch.device('cpu')) #retirar si GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQYkBxllS7j8"
      },
      "source": [
        "## Testing in real time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt2OUWNRS7j8"
      },
      "outputs": [],
      "source": [
        "sequence = []\n",
        "sentence = []\n",
        "predictions = []\n",
        "predictionsArray=[]\n",
        "word=[]\n",
        "threshold = 0.8\n",
        "model_state = 0 #0 for YOLO, 1 for mediapipe\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "className = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J',\n",
        "             'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
        "             'U', 'V', 'W', 'X', 'Y', 'Z']\n",
        "\n",
        "root = Tk()\n",
        "root.title(\"Interpreter\")\n",
        "prediction_label = Label(root, text=\"No hay letra\",padx=1,pady=1, justify=CENTER, font=(\"Arial\", 40))\n",
        "prediction_label.pack()\n",
        "word_label = Label(root, text=\"Palabra\",padx=1,pady=1, justify=CENTER, font=(\"Arial\", 40))\n",
        "word_label.pack()\n",
        "window = timedelta(seconds=1) #tiempo de la ventana\n",
        "initTime=datetime.now()\n",
        "\n",
        "while cap.isOpened():\n",
        "    #Set YOLO model\n",
        "    ret, frame = cap.read()\n",
        "    if model_state == 0:\n",
        "        # Read feed\n",
        "        #results = model_YOLO(frame, conf=0.5) # SI NO HAY GPU\n",
        "        results = model_YOLO.predict(frame, conf=0.5) #Devuelve bounding box, etiqueta y una probabilidad\n",
        "\n",
        "        #guardar las predicciones en un array\n",
        "        for r in results:\n",
        "            boxes = r.boxes\n",
        "            for box in boxes:\n",
        "                cls = int(box.cls[0])\n",
        "                #print(\"predicted class\",className[cls])\n",
        "                predictionsArray.append(className[cls])\n",
        "\n",
        "        #se acaba el tiempo de la ventana\n",
        "        if datetime.now()-initTime >= window:\n",
        "            if predictionsArray:\n",
        "                #se encuentra la letra más común\n",
        "                counter = collections.Counter(predictionsArray)\n",
        "                most_common = counter.most_common(1)[0][0]\n",
        "                word.append(most_common)\n",
        "                prediction_label.config(text=f\"Letra : {most_common}\")\n",
        "                word_label.config(text=f\"{''.join(word)}\")\n",
        "            else:\n",
        "                prediction_label.config(text=\"No hay letra\")\n",
        "                word_label.config(text=f\"{''.join(word)}\")\n",
        "            predictionsArray.clear()\n",
        "            initTime=datetime.now()\n",
        "        root.update()\n",
        "        annotated_frame = results[0].plot()\n",
        "\n",
        "        cv2.imshow(\"YOLOv8 Inference\", annotated_frame)\n",
        "\n",
        "    # Set mediapipe model\n",
        "    else: #model_state == 1: Mediapipe\n",
        "        with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "            # Make detections\n",
        "            image, results = mediapipe_detection(frame, holistic)\n",
        "            print(results)\n",
        "\n",
        "            # Draw landmarks\n",
        "            draw_styled_landmarks(image, results)\n",
        "\n",
        "            # 2. Prediction logic\n",
        "            keypoints = extract_keypoints(results)\n",
        "            sequence.append(keypoints)\n",
        "            sequence = sequence[-30:]\n",
        "\n",
        "            if len(sequence) == 30:\n",
        "                res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
        "                print(actions[np.argmax(res)])\n",
        "                cv2.putText(image, '{}'.format(res), (200,120),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 255), 3, cv2.LINE_AA)\n",
        "                predictions.append(np.argmax(res))\n",
        "\n",
        "            # Show to screen\n",
        "            cv2.imshow('OpenCV Feed', image)\n",
        "\n",
        "    # Break gracefully\n",
        "    key = cv2.waitKey(1) & 0xFF\n",
        "    if key == ord('d'):\n",
        "        # Cambia al otro modelo\n",
        "        cv2.destroyAllWindows()\n",
        "        if model_state == 0:\n",
        "            model_state = 1\n",
        "        else:\n",
        "            model_state = 0\n",
        "        time.sleep(2)\n",
        "        #cv2.namedWindow('Detección de Gestos')\n",
        "\n",
        "    # Sale del bucle si se presiona la tecla 'q'\n",
        "    elif key == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}